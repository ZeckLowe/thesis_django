{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Codes\\Django\\thesis_django\\.backend_venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from tqdm.autonotebook import tqdm\n",
            "c:\\Codes\\Django\\thesis_django\\.backend_venv\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "import hashlib\n",
        "from pinecone import Pinecone\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import firebase_admin\n",
        "import google.cloud\n",
        "from firebase_admin import credentials, firestore\n",
        "from prompt_templates import prompt_templates\n",
        "from google.cloud.firestore_v1.base_query import FieldFilter\n",
        "from sentence_transformers import CrossEncoder\n",
        "from collections import Counter\n",
        "from fuzzywuzzy import fuzz\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*Firestore connected successfully!\n",
            "*Pinecone connected successfully!\n",
            "*OpenAI connected successfully!\n",
            "*CrossEncoder connected successfully!\n"
          ]
        }
      ],
      "source": [
        "load_dotenv()\n",
        "\n",
        "# Firestore Initialization\n",
        "# credential_path = r'C:\\Users\\user\\OneDrive\\Desktop\\thesis_django\\echo_backend\\echo_chatbot\\ServiceAccountKey.json'\n",
        "credential_path = r'C:\\Codes\\Django\\thesis_django\\echo_backend\\echo_chatbot\\ServiceAccountKey.json'\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n",
        "\n",
        "if not firebase_admin._apps:\n",
        "    # cred = credentials.Certificate(r'C:\\Users\\user\\OneDrive\\Desktop\\thesis_django\\echo_backend\\echo_chatbot\\ServiceAccountKey.json')\n",
        "    cred = credentials.Certificate(r'C:\\Codes\\Django\\thesis_django\\echo_backend\\echo_chatbot\\ServiceAccountKey.json')\n",
        "    firebase_admin.initialize_app(cred)\n",
        "\n",
        "try:\n",
        "    db = firestore.Client()\n",
        "    print(\"*Firestore connected successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to connect to Firestore: {e}\")\n",
        "\n",
        "# API Keys Initialization\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY_EVALUATION')\n",
        "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"OpenAI API Key not found!\")\n",
        "if not PINECONE_API_KEY:\n",
        "    print(\"Pinecone API Key not found!\")\n",
        "\n",
        "# Pinecone Initialization\n",
        "try:\n",
        "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "    print(\"*Pinecone connected successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to connect to Pinecone: {e}\")\n",
        "\n",
        "\n",
        "# OpenAI Initialization\n",
        "try:\n",
        "    client=OpenAI(api_key=OPENAI_API_KEY)\n",
        "    LLM = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
        "    EMBEDDINGS = OpenAIEmbeddings(model='text-embedding-3-small')\n",
        "    print(\"*OpenAI connected successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to connect to OpenAI: {e}\")\n",
        "\n",
        "# CrossEncoder Initialization\n",
        "try:\n",
        "    reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
        "    print(\"*CrossEncoder connected successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to connect to CrossEncoder: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"Did they decide to use the Switchboard model as the starting point for their speaker adaptation?\"\n",
        "user_id = \"WuhmTzwTwmerjkSSK4XT8FyJS263\"\n",
        "session_id = \"session1\"\n",
        "organization = \"SCS\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Embeddings\n",
        "def get_embeddings(text):\n",
        "    \"\"\"\n",
        "    This function returns a list of the embeddings for a given query\n",
        "    \"\"\"\n",
        "    text_embeddings = EMBEDDINGS.embed_query(text)\n",
        "    # print(\"Generating Embeddings: Done!\")\n",
        "    return text_embeddings\n",
        "\n",
        "# query_embeddings = get_embeddings(text=query)\n",
        "# print(query_embeddings)\n",
        "# type(query_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def resolve_namespace(query, query_embeddings, summaries):\n",
        "    \"\"\"\n",
        "    Resolves the namespace by selecting the most similar one using fuzzy matching (fuzzywuzzy).\n",
        "    \"\"\"\n",
        "    def ambiguous_fuzzy(query_embeddings, summaries):\n",
        "        \"\"\"\n",
        "        Rank namespaces by semantic similarity to the query.\n",
        "        \"\"\"   \n",
        "        # Compute similarity with meeting summaries\n",
        "        summary_embeddings = {title: get_embeddings(summary) for title, summary in summaries.items()}\n",
        "        print(\"Generated summary embeddings:\", summary_embeddings)\n",
        "\n",
        "        summary_similarities = {\n",
        "            title: cosine_similarity([query_embeddings], [embedding])[0][0] for title, embedding in summary_embeddings.items()\n",
        "        }\n",
        "        print(\"Computed Summary Similarity:\", summary_similarities)\n",
        "\n",
        "        # Rank by similarity\n",
        "        ranked_candidates = sorted(summary_similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "        print(\"\\nðŸ”¹ Initial Ranking (Cosine Similarity):\", ranked_candidates)\n",
        "\n",
        "        score_diff = ranked_candidates[0][1] - ranked_candidates[1][1]\n",
        "        print(\"Score difference:\", score_diff)\n",
        "\n",
        "        if score_diff > 0.2:\n",
        "            print(\"Cosine similarity is clear\")\n",
        "            return ranked_candidates[0][0]\n",
        "        \n",
        "        # Prepare input for re-ranking\n",
        "        cross_encoder_inputs = [(summaries[title], query) for title, _ in ranked_candidates]\n",
        "\n",
        "        # Compute cross-encoder scores\n",
        "        scores = reranker.predict(cross_encoder_inputs)\n",
        "\n",
        "        # Re-rank based on cross-encoder scores\n",
        "        reranked_candidates = sorted(zip(ranked_candidates, scores), key=lambda x: x[1], reverse=True)\n",
        "        print(\"\\nðŸ”¹ Cross Encoder:\", reranked_candidates)\n",
        "\n",
        "        score_diff = reranked_candidates[0][1] - reranked_candidates[1][1]\n",
        "        print(\"Score difference:\", score_diff)\n",
        "\n",
        "        if score_diff < 0.9:\n",
        "            print(\"Ambiguous in Cross Encoder\")\n",
        "            return \"\"\n",
        "\n",
        "        print(\"\\nðŸ”¹ Re-ranked Candidates (Cross-Encoder):\", reranked_candidates)\n",
        "        \n",
        "        return reranked_candidates[0][0][0]\n",
        "\n",
        "    def get_most_similar_namespace(query, query_embeddings, summaries):\n",
        "        \"\"\"\n",
        "        Rank namespaces by fuzzy matching (using fuzzywuzzy's token_set_ratio).\n",
        "        \"\"\"\n",
        "        top_two = {}\n",
        "\n",
        "        similarities = {\n",
        "            title: (fuzz.token_set_ratio(query.lower(), f\"{title}\".lower()) + fuzz.token_set_ratio(query.lower(), f\"{summary}\".lower()))/2\n",
        "            for title, summary in summaries.items()\n",
        "        }\n",
        "\n",
        "        print(\"Computed fuzzy similarities:\", similarities)\n",
        "\n",
        "        # Rank namespaces based on similarity score\n",
        "        ranked_namespaces = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
        "        print(\"Ranked namespaces:\", ranked_namespaces)\n",
        "\n",
        "        # Check for ambiguity\n",
        "        if len(ranked_namespaces) > 1:\n",
        "            diff = ranked_namespaces[0][1] - ranked_namespaces[1][1]\n",
        "            if diff < 15:\n",
        "                print(\"Ambiguous fuzzy match.\")\n",
        "                top_two[ranked_namespaces[0][0]] = summaries.get(ranked_namespaces[0][0])\n",
        "                top_two[ranked_namespaces[1][0]] = summaries.get(ranked_namespaces[1][0])\n",
        "                print(\"Top two:\", top_two)\n",
        "                return ambiguous_fuzzy(query_embeddings, top_two)\n",
        "\n",
        "        return ranked_namespaces[0][0] if ranked_namespaces else \"\"\n",
        "\n",
        "    namespace = get_most_similar_namespace(query, query_embeddings, summaries)\n",
        "    print(f\"Selected namespace: {namespace}\")\n",
        "    return namespace\n",
        "\n",
        "# meeting_title = resolve_namespace(query_embeddings=query_embeddings, organization=organization)\n",
        "# print(meeting_title)\n",
        "# type(meeting_title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Relevant Documents\n",
        "def query_pinecone_index(query_embeddings, meeting_title, index, top_k=3, include_metadata=True):\n",
        "    \"\"\"\n",
        "    Query a Pinecone index.\n",
        "    \"\"\"\n",
        "    # Query Pinecone using the build filter conditions\n",
        "    query_response = index.query(\n",
        "        vector=query_embeddings,\n",
        "        top_k=top_k,\n",
        "        include_metadata=include_metadata,\n",
        "        namespace=meeting_title )\n",
        "\n",
        "    print(\"Querying Pinecone Index: Done!\")\n",
        "    return [match['metadata']['text'] for match in query_response['matches']], [match['metadata']['date'] for match in query_response['matches']], [match['metadata']['title'] for match in query_response['matches']]\n",
        "\n",
        "# index = pc.Index(organization.lower())\n",
        "# text_answers, dates, titles = query_pinecone_index(query_embeddings=query_embeddings, meeting_title=meeting_title, index=index)\n",
        "# print(f\"{text_answers}\\n{dates[0]}\\n{titles[0]}\")\n",
        "# type(text_answers)\n",
        "# type(dates)\n",
        "# type(titles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chat History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_chat_history(user_id, session_id):\n",
        "    \"\"\"\n",
        "    Initializes a chat history object.\n",
        "    \"\"\"\n",
        "    chat_history = []\n",
        "    doc_ref = db.collection(\"chatHistory\").document(user_id).collection(\"session\").document(session_id)\n",
        "    doc_snapshot = doc_ref.get()\n",
        "    try:\n",
        "        if doc_snapshot.exists:\n",
        "            messages = doc_snapshot.get('messages')\n",
        "            if messages is None:\n",
        "                print(f\"No 'messages' field found in document for user_id={user_id}, session_id={session_id}\")\n",
        "                return chat_history\n",
        "            messages = doc_snapshot.get('messages')\n",
        "\n",
        "            for message in messages:\n",
        "                chat_history.append(message)\n",
        "            print(f\"Chat History Initialized: {chat_history}\")\n",
        "        else:\n",
        "            print(f\"No document found for user_id={user_id}, session_id={session_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing chat history: {str(e)}\")\n",
        "    \n",
        "    return chat_history\n",
        "\n",
        "def update_chat_history(user_id, session_id, chat_history):\n",
        "    \"\"\"\n",
        "    Updates the chat history object.\n",
        "    \"\"\"\n",
        "    doc_ref = db.collection(\"chatHistory\").document(user_id).collection(\"session\").document(session_id)\n",
        "    try:\n",
        "        doc_ref.update({\n",
        "            'messages': chat_history\n",
        "        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error updating chat history: {str(e)}\")\n",
        "\n",
        "def process_chat_history(chat_history):\n",
        "    \"\"\"\n",
        "    Changes the chat history list into a HumanMessages and AIMessages Schema\n",
        "    \"\"\"\n",
        "    process_chat_history = []\n",
        "    for idx, message in enumerate(chat_history):\n",
        "        if idx % 2 == 0:\n",
        "            process_chat_history.append(HumanMessage(message))\n",
        "        else:\n",
        "            process_chat_history.append(AIMessage(message))\n",
        "\n",
        "        \n",
        "    return process_chat_history\n",
        "\n",
        "# chat_history = initialize_chat_history(user_id=user_id, session_id=session_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decomposition_query_process(question, text_answers, text_date, text_title, chat_history):\n",
        "    \"\"\"Implements decomposition query\"\"\"\n",
        "\n",
        "    def output_parser(output):\n",
        "        \"\"\"\n",
        "        Helps parses the LLM output, prints it, and returns it.\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + output.content + \"\\n\")\n",
        "\n",
        "        return output.content\n",
        "\n",
        "    def decompose_question(question, chat_history):\n",
        "        \"\"\"\n",
        "        Decomposes a complex question into smaller questions.\n",
        "        \"\"\"\n",
        "        prompt = prompt_templates.decomposition_template().format(question=question, chat_history=chat_history)\n",
        "        response = LLM.invoke(prompt)\n",
        "        subquestions = response.content.split(\"\\n\")\n",
        "        print(\"Decomposing Question: Done!\")\n",
        "\n",
        "        return subquestions\n",
        "    \n",
        "    def generate_qa_pairs(subquestions, context):\n",
        "        \"\"\"Generates QA pairs by answering each subquestion.\"\"\"\n",
        "        qa_pairs = []\n",
        "        for subquestion in subquestions:\n",
        "            context = context\n",
        "            rag_prompt = prompt_templates.qa_template().format(context=context, subquestion=subquestion)\n",
        "            answer = LLM.invoke(rag_prompt)\n",
        "            qa_pairs.append((subquestion, answer))\n",
        "        print(\"Generating QA Pairs: Done!\")\n",
        "\n",
        "        return qa_pairs\n",
        "    \n",
        "    def build_final_answer(question, context, qa_pairs, text_title, text_date, chat_history):\n",
        "        \"\"\"Builds a final answer by integrating the context and QA pairs.\"\"\"\n",
        "        qa_pairs_str = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in qa_pairs])\n",
        "        # final_prompt = prompt_templates.final_rag_template().format(context=context, qa_pairs=qa_pairs_str, question=question)\n",
        "        final_prompt = prompt_templates.final_rag_template_with_memory().format(context=context, qa_pairs=qa_pairs_str, question=question, chat_history=chat_history, text_date=text_date, text_title=text_title)\n",
        "        final_response = LLM.invoke(final_prompt)\n",
        "        print(\"Building Final Answer: Done!\")\n",
        "\n",
        "        return final_response\n",
        "    \n",
        "    subquestions = decompose_question(question, chat_history)\n",
        "    qa_pairs = generate_qa_pairs(subquestions, text_answers)\n",
        "    print(qa_pairs)\n",
        "    final_answer = build_final_answer(question, text_answers, qa_pairs, text_title[0], text_date[0], chat_history)\n",
        "\n",
        "    return output_parser(final_answer)\n",
        "\n",
        "# response = decomposition_query_process(question=query, text_answers=text_answers, chat_history=chat_history)\n",
        "# print(response)\n",
        "# type(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAGAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: Did they decide to use the Switchboard model as the starting point for their speaker adaptation?\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'text_answers' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRelevant Document: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtext_answers\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'text_answers' is not defined"
          ]
        }
      ],
      "source": [
        "print(f\"Query: {query}\")\n",
        "print(f\"Relevant Document: {text_answers}\")\n",
        "print(f\"Answer: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_queries = [\n",
        "    \"Can you summarize the main points of the discussion on remote control design?\",\n",
        "    \"Besides the design, were there any other aspects of the remote control that were discussed?\",\n",
        "    \"What did the Industrial Designer recommend doing regarding the size of the LCD display?\",\n",
        "    \"What were the tasks or next steps agreed upon by the participants at the end of the meeting?\",\n",
        "    \"Did the participants agree on a specific type of material to use for the remote control's casing?\"\n",
        "]\n",
        "\n",
        "expected_responses = [\n",
        "    \"Here are the main points of the discussion on remote control design:\\n\\n* The remote control should be user-friendly and accessible to a wide range of users, including older adults and children.\\n* The remote should have a simple and intuitive design, with clear and easily recognizable buttons.\\n* The remote could have a combination of physical buttons and an LCD display with menus for additional functions.\\n* The remote could have a flip-top design to save space and provide a larger screen for the LCD display.\",\n",
        "    \"Yes, other aspects of the remote control were discussed, including:\\n\\n* The importance of making the remote control affordable to produce, with a production cost of around twelve pounds fifty.\\n* The target market for the remote control is international, with a profit aim of fifty million Euros in the first year.\\n* The remote control should be able to integrate multiple devices, such as TVs, amplifiers, and DVD players, into one device.\",\n",
        "    \"The Industrial Designer did not recommend anything regarding the size of the LCD display.\",\n",
        "    \"The meeting transcript does not contain the answer to this question.\",\n",
        "    \"The participants did not agree on a specific type of material to use for the remote control's casing. They discussed using plastic, but they did not make a final decision.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Querying Pinecone Index: Done!\n",
            "Decomposing Question: Done!\n",
            "Generating QA Pairs: Done!\n",
            "[('1. What are the key design principles for remote controls?', AIMessage(content='The key design principles for remote controls, as discussed in the context, include originality and trendiness to appeal to a wide market, ensuring user-friendliness for a diverse audience ranging from children to the elderly, and even pets. Additionally, the functional design is crucial, focusing on integrating multiple functions into a single device to avoid the clutter of multiple remotes. Reliability is also important, as indicated by the need for a remote that maintains its programming without frequent reconfiguration. Overall, the design should prioritize ease of use and practicality while being visually appealing.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 111, 'prompt_tokens': 386, 'total_tokens': 497, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-07bbd328-135f-4968-adfc-3b4434fdb896-0', usage_metadata={'input_tokens': 386, 'output_tokens': 111, 'total_tokens': 497, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})), ('2. How do user preferences influence remote control design?', AIMessage(content='User preferences significantly influence remote control design by guiding the development of features that cater to a diverse audience. The discussion highlights the importance of creating a remote that is not only functional but also appealing to a wide market, including users of all ages, from children to the elderly, and even pets. This necessitates a user-friendly design that simplifies operation, ensuring that it is intuitive and accessible. Additionally, the desire for an original and trendy product suggests that user preferences also drive aesthetic considerations, pushing designers to innovate beyond conventional designs. The integration of multiple functions into a single remote, as mentioned in the conversation, reflects a preference for convenience and efficiency, indicating that user feedback and experiences with existing products, such as universal remotes that lose signals, inform the design process to enhance reliability and usability.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 158, 'prompt_tokens': 385, 'total_tokens': 543, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-5a3c8075-0b07-44f4-a0cf-9eb05a2a2cf8-0', usage_metadata={'input_tokens': 385, 'output_tokens': 158, 'total_tokens': 543, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})), ('3. What are the latest trends in remote control technology?', AIMessage(content='The latest trends in remote control technology focus on creating user-friendly, multifunctional devices that appeal to a broad audience, including children and the elderly. There is a push for originality and innovation, moving away from traditional designs that are often just metal and buttons. The integration of multiple functions into a single remote control is also a key trend, as seen in the development of universal remotes that can manage various devices simultaneously. However, challenges such as signal loss and the need for frequent reprogramming are being addressed to enhance reliability and user experience. Overall, the emphasis is on making remote controls more accessible and appealing to a diverse market.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 127, 'prompt_tokens': 386, 'total_tokens': 513, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-3ed1df6f-8a65-456e-9771-0dbe9a18a0e2-0', usage_metadata={'input_tokens': 386, 'output_tokens': 127, 'total_tokens': 513, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))]\n",
            "Building Final Answer: Done!\n",
            "\n",
            "The discussion on remote control design focused on creating an original and trendy product that appeals to a wide market, including users of all ages, from children to the elderly, and even pets. Key design principles emphasized the importance of user-friendliness and functionality, aiming to integrate multiple functions into a single device to avoid the clutter of multiple remotes. The conversation also highlighted the need for reliability, as existing universal remotes often face issues like signal loss and the necessity for frequent reprogramming. Overall, the goal is to develop a remote control that is not only practical and easy to use but also visually appealing and innovative.\n",
            "\n",
            "Querying Pinecone Index: Done!\n",
            "Decomposing Question: Done!\n",
            "Generating QA Pairs: Done!\n",
            "[('1. What specific aspects of the remote control were discussed besides the design?', AIMessage(content='Besides the design, the discussion included the functionality of the remote control, specifically its ability to program multiple devices and the issue of losing signals, which required reprogramming. There was also mention of the cost of the remote control, suggesting that a cheaper model might have contributed to its performance issues. Additionally, the idea of incorporating a flip top feature for a larger screen was proposed, indicating a focus on enhancing user experience and convenience.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 87, 'prompt_tokens': 399, 'total_tokens': 486, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-8ff01760-03cf-4b1f-9381-4d4fde85b6ed-0', usage_metadata={'input_tokens': 399, 'output_tokens': 87, 'total_tokens': 486, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})), ('2. Were there any technical features mentioned?', AIMessage(content='Yes, several technical features were mentioned in the context. One feature discussed was a universal remote control that can be programmed to control multiple devices, although it had issues with losing signals and required reprogramming. Additionally, there was a suggestion for a mobile phone design that includes a flip-top remote control, which would allow for a larger screen when flipped open. These features highlight the focus on functionality and user-friendliness in the design of the remote control.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 393, 'total_tokens': 485, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-9e5362de-ce82-4b0f-8b77-66bbd59f2ad5-0', usage_metadata={'input_tokens': 393, 'output_tokens': 92, 'total_tokens': 485, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})), ('3. Did the discussion include user experience or functionality?', AIMessage(content='The discussion included both user experience and functionality. The team emphasized the importance of creating a remote control that is user-friendly for a wide range of users, from children to the elderly, indicating a focus on user experience. Additionally, they discussed functional design aspects, such as the need for the remote to be original and appealing while fulfilling specific needs, which highlights the importance of functionality in their development process.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 80, 'prompt_tokens': 395, 'total_tokens': 475, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c4263850-6c66-41ef-848d-0a470f7f2b85-0', usage_metadata={'input_tokens': 395, 'output_tokens': 80, 'total_tokens': 475, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))]\n",
            "Building Final Answer: Done!\n",
            "\n",
            "Yes, besides the design, the discussion included the functionality of the remote control, specifically its ability to program multiple devices and the issue of losing signals, which required reprogramming. There was also mention of the cost of the remote control, suggesting that a cheaper model might have contributed to its performance issues. Additionally, the idea of incorporating a flip-top feature for a larger screen was proposed, indicating a focus on enhancing user experience and convenience.\n",
            "\n",
            "Querying Pinecone Index: Done!\n",
            "Decomposing Question: Done!\n",
            "Generating QA Pairs: Done!\n",
            "[('1. What specific size did the Industrial Designer recommend for the LCD display?', AIMessage(content='The Industrial Designer recommended keeping the LCD display down to a black and white size, although a specific size was not mentioned in the context provided.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 384, 'total_tokens': 413, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-15119032-bd9a-4e8a-9064-8abcfd4513ca-0', usage_metadata={'input_tokens': 384, 'output_tokens': 29, 'total_tokens': 413, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})), ('2. Were there any reasons provided for this size recommendation?', AIMessage(content='Yes, the discussion suggests that the size recommendation for the screen is influenced by the need to accommodate the older generation. The Industrial Designer mentions that for a lower price point, such as twelve Euros fifty, the screen would likely need to be a black and white LCD, which implies limitations in size and functionality. Additionally, the Marketing representative proposes the idea of incorporating a large print option and a menu display to make the remote control more user-friendly for older users, indicating that ease of use and accessibility are key reasons for the design considerations.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 107, 'prompt_tokens': 381, 'total_tokens': 488, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-5c555eed-d90a-464d-b504-a700eac27a13-0', usage_metadata={'input_tokens': 381, 'output_tokens': 107, 'total_tokens': 488, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})), ('3. Did the Industrial Designer suggest any alternatives to the recommended size?', AIMessage(content='The Industrial Designer did not explicitly suggest any alternatives to the recommended size. Instead, they mentioned that for a lower price point, such as twelve Euros fifty, one would likely have to limit the design to a black and white LCD screen. This implies a constraint rather than an alternative size suggestion.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 383, 'total_tokens': 442, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-4a3c931d-cdb6-40eb-82e1-235a7aeb6801-0', usage_metadata={'input_tokens': 383, 'output_tokens': 59, 'total_tokens': 442, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))]\n",
            "Building Final Answer: Done!\n",
            "\n",
            "The Industrial Designer recommended keeping the LCD display down to a black and white size. This suggestion was influenced by the need to accommodate a lower price point, specifically mentioning twelve Euros fifty, which implies limitations in size and functionality for the display.\n",
            "\n",
            "Querying Pinecone Index: Done!\n",
            "Decomposing Question: Done!\n",
            "Generating QA Pairs: Done!\n",
            "[('1. What specific tasks were assigned to each participant at the end of the meeting?', AIMessage(content='At the end of the meeting, the Project Manager indicated that each participant would go off to do their individual tasks, although specific tasks for each participant were not detailed in the provided context. The focus was on individual work before reconvening for further discussion.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 297, 'total_tokens': 349, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-8742776a-49bb-4f25-b79b-a0714ee64062-0', usage_metadata={'input_tokens': 297, 'output_tokens': 52, 'total_tokens': 349, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})), ('2. What deadlines were established for these tasks?', AIMessage(content='The context does not specify any explicit deadlines for the tasks discussed in the meeting. However, it mentions that the team has \"half an hour before the next meeting\" to work on their individual tasks, indicating a time constraint for that particular session. Additionally, the agenda outlines that they have \"twenty five minutes\" to get to know each other, do tool training, and discuss the project plan and ideas, suggesting a structured timeframe for those activities as well.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 290, 'total_tokens': 383, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-ce6aeb12-f5df-420a-9dbe-098deafeec2c-0', usage_metadata={'input_tokens': 290, 'output_tokens': 93, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})), ('3. Were there any follow-up meetings scheduled to review progress on these tasks?', AIMessage(content='The context provided does not mention any follow-up meetings scheduled to review progress on the tasks discussed. The Project Manager indicates that they will have individual tasks to work on before coming back together, but there is no specific mention of future meetings for progress review.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 296, 'total_tokens': 347, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-53f4c0d3-b725-4daa-885f-2c61a64ffc65-0', usage_metadata={'input_tokens': 296, 'output_tokens': 51, 'total_tokens': 347, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))]\n",
            "Building Final Answer: Done!\n",
            "\n",
            "At the end of the meeting, the Project Manager indicated that each participant would go off to do their individual tasks. However, specific tasks for each participant were not detailed in the provided context. The focus was on individual work before reconvening for further discussion.\n",
            "\n",
            "Querying Pinecone Index: Done!\n",
            "Decomposing Question: Done!\n",
            "Generating QA Pairs: Done!\n",
            "[(\"1. Did the participants agree on a specific type of material for the remote control's casing?\", AIMessage(content=\"The participants did not agree on a specific type of material for the remote control's casing. The Project Manager mentioned that the Telewest remote controls are made of silver plastic, which looks smarter, while also referencing the Sky remote controls that are moulded and look different. However, there was no consensus or specific decision made regarding the material.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 68, 'prompt_tokens': 391, 'total_tokens': 459, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-c375ee38-b338-40b1-a4dd-43757226852a-0', usage_metadata={'input_tokens': 391, 'output_tokens': 68, 'total_tokens': 459, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})), ('2. What were the options considered for the material?', AIMessage(content='The options considered for the material of the remote controls included moulded designs for Sky remote controls and silver plastic for Telewest remote controls, which was noted to look a bit smarter.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 383, 'total_tokens': 420, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-6fc305fa-280e-484f-b7b6-dd78613c04ef-0', usage_metadata={'input_tokens': 383, 'output_tokens': 37, 'total_tokens': 420, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})), ('3. Were there any disagreements among the participants regarding the material choice?', AIMessage(content=\"In the provided context, there do not appear to be any explicit disagreements among the participants regarding the material choice for the remote controls. The Project Manager mentions the differences in appearance between Sky remote controls and Telewest remote controls, suggesting that the Telewest remote controls look smarter due to their silver plastic material. However, the discussion seems to be more about considerations and preferences rather than any conflict or disagreement. The participants acknowledge each other's points without expressing opposing views on the material choice.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 386, 'total_tokens': 481, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-5d7cb09b-c27c-46d7-8a04-e0e15a83bb52-0', usage_metadata={'input_tokens': 386, 'output_tokens': 95, 'total_tokens': 481, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))]\n",
            "Building Final Answer: Done!\n",
            "\n",
            "The participants did not agree on a specific type of material for the remote control's casing. The Project Manager mentioned that the Telewest remote controls are made of silver plastic, which looks smarter, while also referencing the moulded designs of Sky remote controls. However, there was no consensus or specific decision made regarding the material during the discussion.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = []\n",
        "meeting_title = \"Initial Concept Meeting for Universal Remote Design: Goals, User Insights, and Creative Exercises\"\n",
        "index = pc.Index(\"scs\")\n",
        "\n",
        "for query,reference in zip(sample_queries,expected_responses):\n",
        "\n",
        "    text_answers = query_pinecone_index(query_embeddings=get_embeddings(query), meeting_title=meeting_title, index=index)\n",
        "    response = decomposition_query_process(question=query, text_answers=text_answers[0], text_date=text_answers[1], text_title=text_answers[2], chat_history=[])\n",
        "    dataset.append(\n",
        "        {\n",
        "            \"user_input\":query,\n",
        "            \"retrieved_contexts\":text_answers[0],\n",
        "            \"response\":response,\n",
        "            \"reference\":reference\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'user_input': 'Did they decide to use the Switchboard model as the starting point for their speaker adaptation?', 'retrieved_contexts': [\"microphones, you know, to the noise. And that should really improve things, um, further. And then you use those adapted models, which are not speaker adapted but sort of acous you know, channel adapted. | Grad E: Channel adapted. | PhD F: use that as the starting models for your speaker adaptation. | Professor B: Yeah. But the thing is, uh I mean, w when you it depends whether you're ju were just using this as a a starter task for you know, to get things going for conversational or if we're\", \"using. It was that's that was isolated digits. | Grad E: Maybe it's the Bell Gram. Bell Digits. Alright. | Professor B: Um. | PhD F: By the way, I think we can improve these numbers if we care to compr improve them by, um, not starting with the Switchboard models but by taking the Switchboard models and doing supervised adaptation on a small amount of digit data collected in this setting. | Grad E: Yep. | PhD F: Because that would adapt your models to the room acoustics and f for the far - field\", \"thing. | PhD A: Or or some high number. | Professor B: Yeah, sure. Get all these insertions. But I'm saying if you do the same kind of limited thing as people have done in Switchboard evaluations or as a | PhD A: Yeah. Where you know who the speaker is and there's no overlap? And you do just the far - field for those regions? | Professor B: Yeah. Yeah. The same sort of numbers that we got those graphs from. Right? | Grad E: Could we do exactly the same thing that we're doing now, but do it with\", 'one one issue one issue with with that is that um, the system has this, uh, notion of a speaker to which is used in adaptation, variance norm uh, you know, both in, uh, mean and variance normalization and also in the VTL estimation. | Professor B: Mm - hmm. | PhD F: So. | Grad E: Yeah, I noticed the script that extracted it. | PhD F: Do y? Is? So does so th so does does, um, the TI - digits database have speakers that are known? | Grad E: Yep. Yep. | PhD F: And is there is there enough data or a', \"backchannels or something that we did allow to have background speech around it. | PhD D: Yeah. | PhD A: those would be able to do that, | Postdoc C: Sorry. | PhD A: but the rest would be constrained. So, I think we have a version that's pretty good for the native speakers. I don't know yet about the non - native speakers. And, um, we basically also made noise models for the different sort of grouped some of the mouth noises together. Um, so, and then there's a background speech model. And we\", 'occurs, like, a bunch of times. | PhD H: Sh | PhD A: And Chuck\\'s on the lapel here, and he also says \" mixed \" but it\\'s at the last one, and of course the aligner th aligns it everywhere else to everybody else\\'s \" mixed \", | PhD H: Yeah. | PhD A: cuz there\\'s no adaptation yet. So there\\'s I think there\\'s some issues about u We probably want to adapt at least the foreground speaker. But, I guess Andreas tried adapting both the foreground and a background generic speaker, and that\\'s actually a', \"PhD A: even if you consider every other person altogether one person in the meeting, but we'll find out anyway. We were I guess the other thing we're we're I should say is that we're gonna, um try compare this type of overlap analysis to Switchboard, where. | PhD F: And | PhD A: and CallHome, where we have both sides, so that we can try to answer this question of, you know, is there really more overlap in meetings or is it just because we don't have the other channel in Switchboard | Professor\", \"which speaker th it is and how close they are to the PZM? | Professor B: Uh. | PhD A: I don't know how different they are from each other. | PhD F: You want to probably choose the PZM channel that is closest to the speaker. | PhD A: To be best. | PhD D: Yeah. | Grad E: For this particular digit ones, I just picked that one. | PhD A: f | Professor B: Well. | PhD A: OK. So we would then use that one, too, | Grad E: So. | PhD F: Oh, OK. | Professor B: This is kind of central. | PhD A: or? |\", 'modify that script to recognize the, um, speakers, um, in the in the, uh, um, TI - digits database. | Grad E: Right. Right. And that, uh. | PhD F: Or you can fake you can fake names for these waveforms that resemble the names that we use here for the for the meetings. | Grad E: Right. | PhD F: That would be the, sort of probably the safest way to do. | Grad E: I might have to do that anyway to to do because we may have to do an extract to get the amount of data per speaker about right. | PhD F:', 'written and have a little more time, uh, t cloning that reject model and then one copy of it would be adapted to the foreground speaker to capture the rejects in the foreground, like fragments and stuff, and the other copy would be adapted to the background speaker. | PhD A: Right. I mean, in general we actually. | PhD F: And. | PhD A: Right now the words like partial words are reject models and you normally allow those to match to any word. | PhD F: Mm - hmm. | PhD A: But then the background'], 'response': 'No, they did not decide to use the Switchboard model as the starting point for their speaker adaptation. Instead, they discussed the possibility of improving their results by taking the Switchboard models and performing supervised adaptation on a small amount of digit data collected in their specific setting.', 'reference': 'Yes, they decided to use the Switchboard model as the starting point for their speaker adaptation, but they would also adapt the models to the room acoustics.'}, {'user_input': 'What benefit did the lapel microphone provide that made it perform better than expected?', 'retrieved_contexts': [\"again? I mean that it was basically the only thing that was even slightly surprising was that the lapel did so well. Um, and in retrospect that's not as surprising as maybe i it shouldn't have been as surprising as I as as I felt it was. The lapel mike is a very high - quality microphone. And as Morgan pointed out, that there are actually some advantages to it in terms of breath noises and clothes rustling if no one else is talking. | PhD D: Yeah. | PhD F: Exactly. | Grad E: Um, so, uh. | Grad\", \"lapel mikes have any directionality to them? | Professor B: There typically don't, no. | PhD F: Because I I suppose you could make some that have sort of that you have to orient towards your mouth, | Grad E: They have a little bit, | PhD F: and then it would. | Grad E: but they're not noise - cancelling. So, uh. | Professor B: They're they're intended to be omni - directional. | Grad E: Right. | Professor B: And th it's and because you don't know how people are gonna put them on, you know. | PhD\", \"interested in the far - field microphone, uh, problem, I mean. So, you want to you want to That's the obvious thing to try. | Postdoc C: Oh. Oh. | Professor B: Right. | PhD F: Right? Then, eh because you you don't have any. | Postdoc C: Yeah. | PhD F: That's where the most m acoustic mismatch is between the currently used models and the the r the set up here. | Professor B: Right. | PhD F: So. | Professor B: Yeah. So that'd be anoth another interesting data point. | PhD F: Mm - hmm. | Professor\", \"A: Mm - hmm. | Professor B: I mean, if you're not doing something ridiculous like feeding it to a speech recognizer, they they they you know, you can hear the sou hear the sounds just fine. | PhD A: Right. | Professor B: You know, it's They I mean, i it's more or less the same principles as these other mikes are built under, it's just that there's less quality control. They just, you know, churn them out and don't check them. Um. So. So that was Yeah. So that was i interesting result. So like I\", \"of the natural. | PhD D: Yeah. You don't move much during reading digits, I think. | Professor B: Yeah. | Grad E: So. | Professor B: Yeah. | Grad E: Right. | Grad G: Probably the fact that it picks up other people's speakers other people's talking is an indication of that it the fact it is a good microphone. | PhD D: Yeah. | Professor B: Right. So in the digits, in most most cases, there weren't other people talking. | Grad E: Right. Right. | Grad G: So. | Professor B: So. | PhD F: D do the\", \"part of it is just the distance. | PhD A: And aren't these pretty bad microphones? | Grad E: Yep. | PhD A: I mean. | Professor B: Well, they're bad. But, I mean, if you listen to it, it sounds OK. You know? u Yeah. | Grad E: Yeah. When you listen to it, uh, the PZM and the PDA Yeah, th the PDA has higher sound floor but not by a lot. It's really pretty uh, pretty much the same. | PhD A: I just remember you saying you got them to be cheap on purpose. Cheap in terms of their quality. So. |\", \"microphones, you know, to the noise. And that should really improve things, um, further. And then you use those adapted models, which are not speaker adapted but sort of acous you know, channel adapted. | Grad E: Channel adapted. | PhD F: use that as the starting models for your speaker adaptation. | Professor B: Yeah. But the thing is, uh I mean, w when you it depends whether you're ju were just using this as a a starter task for you know, to get things going for conversational or if we're\", 'Actually, not randomly. | PhD A: Not randomly. | PhD F: We knew we knew that it had these insertion errors from. | PhD A: It had sort of average recognition performance in a bunch of speakers | PhD F: Yeah. Yeah. | PhD A: and it was a Meeting Recorder meeting. Um. But, yeah, we should try to use what you have. I did re - run recognition on your new version of MR one. | Postdoc C: Oh, good. | PhD A: I I mean the the one with Dan Ellis in it and Eric. | Postdoc C: Good! Uh - huh. Yeah, exactly.', \"finally I did it using the speakers of my, um of you know, off the CPU on my on my machine cuz I didn't have a headphone. | PhD A: Right. | Postdoc C: So it @ @, like, I mean Yeah, I I mean, i in retrospect it would've been good to ha have got I should've gotten a headphone. But in any case, um, thi this is this was transcribed in a in a, uh, less optimal way than than the ones that came after it, and I was able to you know, an and this meant that there were some speaker identif identifications\", \"G: Mm - hmm. | Professor B: Well, it's Yeah, sort of the bre the breath noises and the mouth clicks and so forth like that, the lapel's gonna be better on. | Grad G: It's g it. | PhD D: Or the cross - talk. Yeah. | Professor B: The lapel is typically worse on the on clothes rustling, but if no one's rustling their clothes, | Grad E: Right. I mean, a lot of people are just sort of leaning over and reading the digits, | Professor B: it's it's. | Grad E: so it's it's a very different task than sort\"], 'response': \"The lapel microphone provided several benefits that contributed to its better-than-expected performance. Its high-quality design allowed it to effectively capture sound while minimizing issues such as breath noises and clothing rustling, particularly in situations where no other sounds were present. Additionally, being typically omni-directional, the lapel microphone could pick up sound from various angles, which is advantageous when speakers are not positioned directly in front of it. This combination of features enabled the lapel microphone to deliver clear audio, especially during tasks like reading digits, where it was able to isolate the speaker's voice despite the potential for cross-talk.\", 'reference': \"Lapel microphone had a very high-quality microphone. It picks up other people's speakers other people's talking is an indication of that it the fact it is a good microphone. Some advantages to it in terms of breath noises and clothes rustling if no one else is talking.\"}, {'user_input': 'What did PhD A recommend to do when discussing forced alignment?', 'retrieved_contexts': [\"Grad E: Uh - huh. | PhD F: Just like i th like th as if there were these waveform segments, but instead you have sort of feature file segments. But, you know So. | Professor B: Cool. OK. So the s the the next thing we had on the agenda was something about alignments? | PhD A: Oh. Yes, we have I don't know, did you wanna talk about it, or? I can give a I was just telling this to Jane and and W we we were able to get some definite improvement on the forced alignments by looking at them first and\", \"| PhD F: Talk about aligning people's schedules. | Professor B: Yeah. | Grad E: Yeah. | Postdoc C: Mm - hmm. | Professor B: Yeah. I mean Right. Yeah, I mean, it was. | Grad E: Yeah, it's forced alignment of people's schedules. | PhD F: Yeah. | PhD D: Forced align. | PhD F: If we're very. | Professor B: Yeah. | PhD F: Yeah. | Professor B: With with whatever it was, a month and a half or something ahead of time, the only time we could find in common roughly in common, was on a Saturday. | PhD D:\", 'Right. Um, but it turned out for for to get accurate alignments it was really important to open up the pruning significantly. | PhD A: Right. | Professor B: Hmm. | PhD F: Um because otherwise it would sort of do greedy alignment, um, in regions where there was no real speech yet from the foreground speaker. | Professor B: Mm - hmm. | PhD F: Um, so that was one big factor that helped improve things and then the other thing was that, you know, as Liz said the we f enforce the fact that, uh, the', 'So. | Professor B: Uh, if we We have two. | Postdoc C: We have two. | Professor B: Yeah. Just ha uh, trying out the alignment procedure that you have on that | PhD A: Mm - hmm. | Professor B: you could actually get something, um uh, uh, get an objective measure. Uh. | PhD F: Mm - hmm. | PhD A: You mean on on the hand - marked, um So we we only r hav I only looked at actually alignments from one meeting that we chose, | Professor B: Yeah. | PhD A: I think MR four, just randomly, um And. | PhD F:', 'occurs, like, a bunch of times. | PhD H: Sh | PhD A: And Chuck\\'s on the lapel here, and he also says \" mixed \" but it\\'s at the last one, and of course the aligner th aligns it everywhere else to everybody else\\'s \" mixed \", | PhD H: Yeah. | PhD A: cuz there\\'s no adaptation yet. So there\\'s I think there\\'s some issues about u We probably want to adapt at least the foreground speaker. But, I guess Andreas tried adapting both the foreground and a background generic speaker, and that\\'s actually a', \"talking about. | Professor B: Yeah. | PhD F: The, um th the other good thing about the alignments is that, um, it's not always the machine's fault if it doesn't work. So, you can actually find, um, | PhD A: It's the person's fault. | PhD F: problem uh, proble | PhD A: It's Morgan's fault. | PhD F: You can find. | Professor B: It's always Morgan's fault. | PhD F: You can find, uh, problems with with the transcripts, um, you know, | Grad E: Oh. | PhD A: Yeah. | PhD F: and go back and fix them. |\", \"Professor B: G given I I mean, I wa I wa I was gonna ask you anyway, uh, how you assessed that things were better. | PhD F: Mm - hmm. | PhD A: I looked at them. I spent two days um, in Waves. | Professor B: OK. | PhD A: Oh, it was painful because the thing is, you know the alignments share a lot in common, so And you're yo you're looking at these segments where there's a lot of speech. I mean, a lot of them have a lot of words. Not by every speaker | Professor B: Yeah. | PhD A: but by some\", \"a far - field mike? | Professor B: Yeah, do it with one of on | Grad E: Cuz we extract the times from the near - field mike, but you use the acoustics from the far - field mike. | PhD A: Right. I understand that. I just meant that so you have three choices. There's, um You can use times where that person is talking only from the transcripts but the segmentations were were synchronized. Or you can do a forced alignment on the close - talking to determine that, the you know, within this segment,\", \"wanna know where those are relative to the spurt overlaps. | PhD F: Mm - hmm. Right. | PhD A: sp overlaps, | PhD F: So so those are actually sort of retro - fitted into the time alignment. | PhD A: or. | PhD F: And then we merge all the alignments from the various channels and we sort them by time. And then there's a then there's a process where you now determine the spurts. That is Actually, no, you do that before you merge the various channels. So you you id identify by some criterion, which\", \"know, tuning yet another yet another parameter. | Grad E: The ve level. Yeah. | PhD A: Yeah. | PhD F: And really the reason we can't do it is just that we don't have a we don't have ground truth for these. So, we would need a hand - marked, um, word - level alignments or at least sort of the boundaries of the speech betw you know, between the speakers. Um, and then use that as a reference and tune the parameters of the of the model, uh, to op to get the best performance. | PhD A: Yeah. |\"], 'response': 'PhD A recommended several strategies for discussing forced alignment. First, they emphasized the importance of conducting an initial review of the forced alignments to identify areas for improvement before making any adjustments. They also highlighted the need to adapt the alignment process for at least the foreground speaker, and potentially a background generic speaker, to enhance accuracy. PhD A shared their experience of spending time analyzing the alignments in detail, particularly focusing on segments with a lot of speech, which is crucial for assessing the effectiveness of the alignments. Additionally, they pointed out that issues with the transcripts can significantly affect alignment accuracy, suggesting that it is essential to check and correct any transcript errors. Lastly, PhD A discussed the process of merging alignments from various channels and sorting them by time to help determine overlaps and improve the overall alignment process. These strategies reflect a systematic approach to enhancing forced alignment through careful analysis, adaptation, and correction of transcripts.', 'reference': 'PhD A suggested using a proximity constraint to help the forced alignment algorithm determine which words most likely correspond to each speaker. They also proposed to allow for reject models that could match fragments and noise. Additionally, PhD A suggested marking the beginning and end of foreground speech by hand to improve the accuracy of the alignment.'}, {'user_input': 'Can you list the main tasks that were agreed upon during the meeting?', 'retrieved_contexts': [\"Grad E: OK, we're on. | Professor B: OK. | Grad E: So, I mean, everyone who's on the wireless check that they're on. | PhD F: C we. | Grad G: Alright. | Postdoc C: I see. Yeah. | PhD F: Yeah. | Grad E: OK, our agenda was quite short. | Professor B: Oh, could you close the door, maybe? Yeah. | Grad E: Sure. Two items, which was, uh, digits and possibly stuff on on, uh, forced alignment, which Jane said that Liz and Andreas had in information on, | Grad E: but they didn't, | PhD F: Mm - hmm. |\", \"Professor B: I guess the only other thing, uh, for which I. | Grad E: so. | PhD F: We should do that second, because Liz might join us in time for that. | Grad E: OK. | Professor B: Um. OK, so there's digits, alignments, and, um, I guess the other thing, which I came unprepared for, uh, is, uh, to dis s s see if there's anything anybody wants to discuss about the Saturday meeting. | Grad E: Right. | Professor B: So. Any I mean, maybe not. | Grad E: Digits and alignments. But. | Professor B: Uh.\", \"should do in prep for Saturday? Um I guess everybody knows about I mean, u um, Mari was asking was trying to come up with something like an agenda and we're sort of fitting around people's times a bit. But, um, clearly when we actually get here we'll move things around this, as we need to, but so you can't absolutely count on it. | PhD D: OK. | Professor B: But but, uh. | PhD D: Yeah. | PhD A: Are we meeting in here probably or? OK. | Professor B: Yeah. That was my thought. | PhD A: Yeah. |\", \"our Yeah, our. | PhD F: Maybe the sections that are not right afte you know, after lunch when everybody's still munching and. | PhD A: So can you send out a schedule once you know it, jus? | Professor B: OK. Well. | PhD A: Is is there a r? | Professor B: OK. Yeah. I guess I sent it around a little bit. | PhD A: There's a res Is it changed now, or? | Professor B: But I hadn't heard back from Mari after I I u u uh, brought up the point abou about Andreas's schedule. So, um, maybe when I get back\", \"of words with the begin times for every word and the duration. | PhD A: It looks like a Waves label file almost. Right? | PhD F: And and and of course. | PhD A: It's just. | PhD F: Right. But it has one the first column has the meeting name, so it could actually contain several meetings. Um. And the second column is the channel. Third column is the, um, start times of the words and the fourth column is the duration of the words. And then we're, um OK. Then we have a messy alignment process where\", \"there'll be some some mail from her. | PhD A: OK. | Professor B: So, I'll make a. | Postdoc C: I'm looking forward to seeing your representation. That'd be, uh. | PhD A: And w we should get the two meetings from y | Postdoc C: I'd like to see that. Yeah. | PhD A: I mean, I know about the first meeting, um, but the other one that you did, the NSA one, which we hadn't done cuz we weren't running recognition on it, because the non - native speaker. | Postdoc C: Mm - hmm. | PhD A: there were five\", \"| PhD F: Talk about aligning people's schedules. | Professor B: Yeah. | Grad E: Yeah. | Postdoc C: Mm - hmm. | Professor B: Yeah. I mean Right. Yeah, I mean, it was. | Grad E: Yeah, it's forced alignment of people's schedules. | PhD F: Yeah. | PhD D: Forced align. | PhD F: If we're very. | Professor B: Yeah. | PhD F: Yeah. | Professor B: With with whatever it was, a month and a half or something ahead of time, the only time we could find in common roughly in common, was on a Saturday. | PhD D:\", \"B: Mm - hmm. | Grad E: Mm - hmm. | PhD A: and we don't know what people are doing. Try to create a paper out of that. | Professor B: Yeah. I mean, y y you folks have probably already told me, but were were you intending to do a Eurospeech submission, or? | PhD A: Um, you mean the one due tomorrow? | Professor B: Yeah. | PhD A: Yeah. Well, we're still, like, writing the scripts for doing the research, and we will Yes, we're gonna try. | Postdoc C: Mm - hmm. | PhD A: And I was telling Don, do not\", \"which were changes. | Grad G: Well, I know there were some speaker labelling problems, um, after interruptions. | Postdoc C: Yeah. Fixed that. | Grad G: Is that what you're referring to? I mean, cuz there's this one instance when, for example, you're running down the stairs. | Postdoc C: Oh, well. | Grad G: I remember this meeting really well. | PhD D: Yeah. | PhD A: Don Don has had He knows he can just read it like a play. | Grad G: Right. It's a Yeah, I've I've I'm very well acquainted with\", \"meeting about meetings. | Grad E: Oh, that's right. Tomorrow. | PhD H: Yeah. | PhD D: The last meeting meeting? | PhD H: Because, eh, I leave, eh, the next Sunday. | Grad E: It's off. | PhD A: Oh. | PhD F: Mm - hmm. | PhD H: I will come back to home to Spain. | Professor B: Yeah. | PhD A: Oh. | Professor B: I d so I I jus | PhD F: Mm - hmm. | PhD H: And I I would like to to to say thank you very much, eh, to all people in the group and at ICSI, | PhD F: Mm - hmm. | Grad E: Yeah. It was good\"], 'response': 'The main tasks agreed upon during the meeting included discussing \"digits\" and \"forced alignment,\" as well as preparing for an upcoming Saturday meeting. Additionally, there was a mention of aligning schedules and possibly creating an agenda for the Saturday meeting.', 'reference': \"The participants decided to conduct a test to determine if the SRI system would show improvement if trained on TI digits, and to compare the results to the system trained on Switchboard data. They also agreed to investigate methods for improving the SRI system's performance in noisy conditions and to explore techniques for enhancing the accuracy of forced alignment.\"}, {'user_input': 'What did Professor B recommend to do during the discussion of digits?', 'retrieved_contexts': [\"using. It was that's that was isolated digits. | Grad E: Maybe it's the Bell Gram. Bell Digits. Alright. | Professor B: Um. | PhD F: By the way, I think we can improve these numbers if we care to compr improve them by, um, not starting with the Switchboard models but by taking the Switchboard models and doing supervised adaptation on a small amount of digit data collected in this setting. | Grad E: Yep. | PhD F: Because that would adapt your models to the room acoustics and f for the far - field\", \"E: Ye - we and we'd have to force you to read lots and lots of digits, | Professor B: and. | Grad E: so it could get real real car noise. | PhD F: Oh, yeah. | PhD D: Yeah. | PhD F: Oh, yeah. | Grad G: Take advantage. | PhD D: And with the kids in the background. | PhD F: I'll let I'd let. | PhD D: Yeah. | PhD F: I let, uh, my five - year - old have a try at the digits, eh. | Professor B: Yeah. | Grad E: So, anyway, I can talk about digits. Um, did everyone get the results or shall I go over them\", \"No, no. | Grad E: So are we gonna do digits simultaneously | PhD A: You This is our reward if we do our digi | Professor B: Well? Yeah. | Postdoc C: OK. | PhD D: Yeah. | Grad E: or what? | PhD D: Simultaneous digit chocolate task. | PhD H: I I think, eh, it's enough, eh, for more peopl for more people after. | Professor B: We're gonna we're gonna do digits at the same. | PhD A: Oh. | PhD F: Mmm! | Postdoc C: That's nice. | PhD H: But, eh. | PhD F: Mm - hmm. | PhD A: Oh, thanks, Jose. | Professor\", \"Professor B: I guess the only other thing, uh, for which I. | Grad E: so. | PhD F: We should do that second, because Liz might join us in time for that. | Grad E: OK. | Professor B: Um. OK, so there's digits, alignments, and, um, I guess the other thing, which I came unprepared for, uh, is, uh, to dis s s see if there's anything anybody wants to discuss about the Saturday meeting. | Grad E: Right. | Professor B: So. Any I mean, maybe not. | Grad E: Digits and alignments. But. | Professor B: Uh.\", \"really interested i in connected digits. And I I think the answer is both. And for for connected digits over the telephone you don't actually want to put a whole lot of effort into adaptation | PhD F: Well, I don't know. | Professor B: because somebody gets on the phone and says a number and then you just want it. You don't don't, uh. | Postdoc C: This is this that one's better. | PhD F: Right. | Postdoc C: Mm - hmm. | PhD F: Um, but, you know, I uh, my impression was that you were actually\", \"Uh - huh. | Grad E: The other thing is, isn't TI - digits isolated digits? | PhD F: Right. | Grad E: Or is that another one? I'm I looked through a bunch of the digits t corp corpora, and now they're all blurring. | Professor B: Mm - hmm. | Grad E: Cuz one of them was literally people reading a single digit. And then others were connected digits. | Professor B: Yeah. Most of TI - digits is connected digits, I think. | Grad E: OK. | Professor B: The I mean, we had a Bellcore corpus that we were\", \"Yeah. | Postdoc C: Yeah. | PhD F: Keep in touch. | PhD H: Thank you. | Professor B: Yeah. OK. I guess, uh, unless somebody has something else, we'll read read our digits | Grad E: Digits? | Professor B: and we'll get our. | PhD D: Uh. | Professor B: get our last bit of, uh, Jose's Jose Jose's digit. | PhD D: Oops. | Grad E: Are we gonna do them simultaneously or? | PhD H: You eh. | Professor B: Uh, I'm sorry? | PhD H: Ye - ye you prefer, eh, to eat, eh, chocolate, eh, at the coffee break, eh, at\", 'Adam knows how to run it, | Professor B: Yeah. | Grad E: Yeah. No problem. | PhD F: so you just make a f | Professor B: Yeah. Yeah. Cuz our sense from the other from the Aurora, uh, task is that. | Grad E: And try it with TI - digits? | PhD F: Mm - hmm. | Professor B: I mean, cuz we were getting sub one percent numbers on TI - digits also with the tandem thing. | PhD F: Mm - hmm. | Professor B: So, one so there were a number of things we noted from this. | PhD F: Mmm. | Professor B: One is,', \"someone just read the wrong digit, I corrected it. | Professor B: Yeah. | Grad E: And then there was another one where Jose couldn't tell whether I couldn't tell whether he was saying zero or six. And I asked him and he couldn't tell either. | Grad I: Hmm. | Grad E: So I just cut it out. | Professor B: Yeah. | Grad E: You know, so I just e edited out the first, i uh, word of the utterance. Um, so there's a little bit of correction but it's definitely not as clean as TI - digits. So my\", \"could and they are using a system that's, um you know, h is actually trained on digits, um, but h h otherwise uses the same, you know, decoder, the same, uh, training methods, and so forth, | Professor B: Mm - hmm. | PhD F: and I could ask them what they get on TI - digits. | Professor B: Yeah, bu although I'd be I think it'd be interesting to just take this exact actual system so that these numbers were comparable | PhD F: Mm - hmm. | Professor B: and try it out on TI - digits. | PhD F: Well,\"], 'response': 'During the discussion of digits, Professor B recommended that the participants should read their digits and suggested the possibility of conducting the digit tasks simultaneously. He emphasized the importance of adapting models to the room acoustics to improve performance. Additionally, he expressed interest in discussing the results of their digit readings and noted that most TI-digits are connected digits, which could influence their approach to the task.', 'reference': 'Professor B recommended running a test to determine whether the SRI system would show improvement if trained on TI digits, and to compare the results to the system trained on Switchboard data.'}]\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import EvaluationDataset\n",
        "evaluation_dataset = EvaluationDataset.from_list(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:29<00:00,  1.18s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'faithfulness': 0.7750, 'factual_correctness': 0.2680, 'context_precision': 1.0000, 'answer_relevancy': 0.5076, 'context_recall': 0.3000}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(LLM)\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(EMBEDDINGS)\n",
        "from ragas.metrics import Faithfulness, FactualCorrectness, ContextPrecision, ResponseRelevancy, LLMContextRecall\n",
        "\n",
        "result = evaluate(dataset=evaluation_dataset,metrics=[Faithfulness(), FactualCorrectness(), ContextPrecision(), ResponseRelevancy(), LLMContextRecall()],llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results uploaded! View at https://app.ragas.io/dashboard/alignment/evaluation/6da27426-581f-4bf3-9395-642a4aa83ffc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'https://app.ragas.io/dashboard/alignment/evaluation/6da27426-581f-4bf3-9395-642a4aa83ffc'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.environ[\"RAGAS_APP_TOKEN\"] = \"apt.4f8e-299f727bff54-c9b1-b0b4-f480bd0c-ea62a\"\n",
        "\n",
        "result.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.841988900417566"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas import SingleTurnSample\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas.metrics import ResponseRelevancy\n",
        "\n",
        "sample = SingleTurnSample(\n",
        "    user_input=query,\n",
        "    response=response,\n",
        "    retrieved_contexts=text_answers,\n",
        ")\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(LLM)\n",
        "evaluator_embeddings = LangchainEmbeddingsWrapper(EMBEDDINGS)\n",
        "scorer = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\n",
        "await scorer.single_turn_ascore(sample)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".backend_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
