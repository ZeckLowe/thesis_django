{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52055cba",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c7b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "import hashlib\n",
    "from pinecone import Pinecone\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import firebase_admin\n",
    "import google.cloud\n",
    "from firebase_admin import credentials, firestore\n",
    "from .prompt_templates import prompt_templates\n",
    "from google.cloud.firestore_v1.base_query import FieldFilter\n",
    "from sentence_transformers import CrossEncoder\n",
    "from collections import Counter\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691595c4",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firestore Initialization\n",
    "# credential_path = r'C:\\Users\\user\\OneDrive\\Desktop\\thesis_django\\echo_backend\\echo_chatbot\\ServiceAccountKey.json'\n",
    "credential_path = r'C:\\Codes\\Django\\thesis_django\\echo_backend\\echo_chatbot\\ServiceAccountKey.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n",
    "\n",
    "if not firebase_admin._apps:\n",
    "    # cred = credentials.Certificate(r'C:\\Users\\user\\OneDrive\\Desktop\\thesis_django\\echo_backend\\echo_chatbot\\ServiceAccountKey.json')\n",
    "    cred = credentials.Certificate(r'C:\\Codes\\Django\\thesis_django\\echo_backend\\echo_chatbot\\ServiceAccountKey.json')\n",
    "    firebase_admin.initialize_app(cred)\n",
    "\n",
    "try:\n",
    "    db = firestore.Client()\n",
    "    print(\"*Firestore connected successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Firestore: {e}\")\n",
    "\n",
    "# API Keys Initialization\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"OpenAI API Key not found!\")\n",
    "if not PINECONE_API_KEY:\n",
    "    print(\"Pinecone API Key not found!\")\n",
    "\n",
    "# Pinecone Initialization\n",
    "try:\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    print(\"*Pinecone connected successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Pinecone: {e}\")\n",
    "\n",
    "\n",
    "# OpenAI Initialization\n",
    "try:\n",
    "    client=OpenAI(api_key=OPENAI_API_KEY)\n",
    "    LLM = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "    EMBEDDINGS = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    print(\"*OpenAI connected successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to OpenAI: {e}\")\n",
    "\n",
    "# CrossEncoder Initialization\n",
    "try:\n",
    "    reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L6-v2\")\n",
    "    print(\"*CrossEncoder connected successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to CrossEncoder: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0b1757",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902478aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Embeddings\n",
    "def get_embeddings(text):\n",
    "    \"\"\"\n",
    "    This function returns a list of the embeddings for a given query\n",
    "    \"\"\"\n",
    "    text_embeddings = EMBEDDINGS.embed_query(text)\n",
    "    print(\"Generating Embeddings: Done!\")\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb60784",
   "metadata": {},
   "source": [
    "# Resolve Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_namespace(query, query_embeddings, summaries):\n",
    "    \"\"\"\n",
    "    Resolves the namespace by selecting the most similar one using fuzzy matching (fuzzywuzzy).\n",
    "    \"\"\"\n",
    "    def ambiguous_fuzzy(query_embeddings, summaries):\n",
    "        \"\"\"\n",
    "        Rank namespaces by semantic similarity to the query.\n",
    "        \"\"\"   \n",
    "        # Compute similarity with meeting summaries\n",
    "        summary_embeddings = {title: get_embeddings(summary) for title, summary in summaries.items()}\n",
    "        print(\"Generated summary embeddings:\", summary_embeddings)\n",
    "\n",
    "        summary_similarities = {\n",
    "            title: cosine_similarity([query_embeddings], [embedding])[0][0] for title, embedding in summary_embeddings.items()\n",
    "        }\n",
    "        print(\"Computed Summary Similarity:\", summary_similarities)\n",
    "\n",
    "        # Rank by similarity\n",
    "        ranked_candidates = sorted(summary_similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(\"\\nðŸ”¹ Initial Ranking (Cosine Similarity):\", ranked_candidates)\n",
    "        \n",
    "        score_diff = ranked_candidates[0][1] - ranked_candidates[1][1]\n",
    "        print(\"Score difference:\", score_diff)\n",
    "\n",
    "        if score_diff > 0.2:\n",
    "            print(\"Cosine similarity is clear\")\n",
    "            return ranked_candidates[0][0]\n",
    "        \n",
    "        # Prepare input for re-ranking\n",
    "        cross_encoder_inputs = [(summaries[title], query) for title, _ in ranked_candidates]\n",
    "\n",
    "        # Compute cross-encoder scores\n",
    "        scores = reranker.predict(cross_encoder_inputs)\n",
    "\n",
    "        # Re-rank based on cross-encoder scores\n",
    "        reranked_candidates = sorted(zip(ranked_candidates, scores), key=lambda x: x[1], reverse=True)\n",
    "        print(\"\\nðŸ”¹ Cross Encoder:\", reranked_candidates)\n",
    "\n",
    "        score_diff = reranked_candidates[0][1] - reranked_candidates[1][1]\n",
    "        print(\"Score difference:\", score_diff)\n",
    "\n",
    "        if score_diff < 0.9:\n",
    "            print(\"Ambiguous in Cross Encoder\")\n",
    "            return \"\"\n",
    "\n",
    "        print(\"\\nðŸ”¹ Re-ranked Candidates (Cross-Encoder):\", reranked_candidates)\n",
    "        \n",
    "        return reranked_candidates[0][0][0]\n",
    "\n",
    "    def get_most_similar_namespace(query, query_embeddings, summaries):\n",
    "        \"\"\"\n",
    "        Rank namespaces by fuzzy matching (using fuzzywuzzy's token_set_ratio).\n",
    "        \"\"\"\n",
    "        top_two = {}\n",
    "\n",
    "        similarities = {\n",
    "            title: (fuzz.token_set_ratio(query.lower(), f\"{title}\".lower()) + fuzz.token_set_ratio(query.lower(), f\"{summary}\".lower()))/2\n",
    "            for title, summary in summaries.items()\n",
    "        }\n",
    "\n",
    "        print(\"Computed fuzzy similarities:\", similarities)\n",
    "\n",
    "        # Rank namespaces based on similarity score\n",
    "        ranked_namespaces = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "        print(\"Ranked namespaces:\", ranked_namespaces)\n",
    "\n",
    "        # Check for ambiguity\n",
    "        if len(ranked_namespaces) > 1:\n",
    "            diff = ranked_namespaces[0][1] - ranked_namespaces[1][1]\n",
    "            if diff < 15:\n",
    "                print(\"Ambiguous fuzzy match.\")\n",
    "                top_two[ranked_namespaces[0][0]] = summaries.get(ranked_namespaces[0][0])\n",
    "                top_two[ranked_namespaces[1][0]] = summaries.get(ranked_namespaces[1][0])\n",
    "                print(\"Top two:\", top_two)\n",
    "                return ambiguous_fuzzy(query_embeddings, top_two)\n",
    "\n",
    "        return ranked_namespaces[0][0] if ranked_namespaces else \"\"\n",
    "\n",
    "    namespace = get_most_similar_namespace(query, query_embeddings, summaries)\n",
    "    print(f\"Selected namespace: {namespace}\")\n",
    "    return namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158a65ef",
   "metadata": {},
   "source": [
    "# Generate Follow up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd56f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_followup_question(question, meeting_summaries):\n",
    "    \"\"\"\n",
    "    Generate followup response based on the previous query.\n",
    "    \"\"\"\n",
    "    followup_prompt = prompt_templates.followup_template().format(question=question, meeting_list=meeting_summaries)\n",
    "    followup_response = LLM.invoke(followup_prompt)\n",
    "    print(\"Generating followup question: Done!\")\n",
    "\n",
    "    return followup_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f16e7",
   "metadata": {},
   "source": [
    "# Query Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60342022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_pinecone_index(query_embeddings, meeting_title, index, top_k=5, include_metadata=True):\n",
    "    \"\"\"\n",
    "    Query a Pinecone index.\n",
    "    \"\"\"\n",
    "    # Build filter conditions directly for Pinecone\n",
    "    filter_conditions = {}\n",
    "\n",
    "    # Include date and meeting title if specified\n",
    "    if meeting_title.lower() != 'unknown':\n",
    "        filter_conditions['title'] = meeting_title\n",
    "\n",
    "    # Query Pinecone using the build filter conditions\n",
    "    query_response = index.query(\n",
    "        vector=query_embeddings,\n",
    "        filter=filter_conditions,\n",
    "        top_k=top_k,\n",
    "        include_metadata=include_metadata,\n",
    "        namespace=meeting_title )\n",
    "\n",
    "    print(\"Querying Pinecone Index: Done!\")\n",
    "    return \" \".join([doc['metadata']['text'] for doc in query_response['matches']]), [doc['metadata']['date'] for doc in query_response['matches']], [doc['metadata']['title'] for doc in query_response['matches']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924448ae",
   "metadata": {},
   "source": [
    "# Fetch summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a64fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_summaries_by_organization(organization):\n",
    "        \"\"\"\n",
    "        Fetches summaries by organization\n",
    "        \"\"\"\n",
    "        summaries = {}\n",
    "        meetings_ref = db.collection(\"Meetings\")\n",
    "        query = meetings_ref.where(filter=FieldFilter(\"organization\", \"==\", organization))\n",
    "        docs = query.stream()\n",
    "\n",
    "        for doc in docs:\n",
    "            data = doc.to_dict()\n",
    "            meeting_title = data.get(\"meetingTitle\")\n",
    "            summary = data.get(\"meetingSummary\")\n",
    "            if meeting_title and summary:\n",
    "                summaries[meeting_title] = summary\n",
    "        \n",
    "        print(f\"Fetched summaries for organization '{organization}': {summaries}\")\n",
    "        return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47de491",
   "metadata": {},
   "source": [
    "# Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72ccc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomposition_query_process(question, text_answers, chat_history, text_date, text_title):\n",
    "    \"\"\"Implements decomposition query\"\"\"\n",
    "\n",
    "    def output_parser(output):\n",
    "        \"\"\"\n",
    "        Helps parses the LLM output, prints it, and returns it.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + output.content + \"\\n\")\n",
    "\n",
    "        return output.content\n",
    "    \n",
    "    def decompose_question(question, chat_history):\n",
    "        \"\"\"\n",
    "        Decomposes a complex question into smaller questions.\n",
    "        \"\"\"\n",
    "        prompt = prompt_templates.decomposition_template().format(question=question, chat_history=chat_history)\n",
    "        response = LLM.invoke(prompt)\n",
    "        subquestions = response.content.split(\"\\n\")\n",
    "        print(\"Decomposing Question: Done!\")\n",
    "\n",
    "        return subquestions\n",
    "    \n",
    "    def generate_qa_pairs(subquestions, context):\n",
    "        \"\"\"Generates QA pairs by answering each subquestion.\"\"\"\n",
    "        qa_pairs = []\n",
    "        for subquestion in subquestions:\n",
    "            context = context\n",
    "            rag_prompt = prompt_templates.qa_template().format(context=context, subquestion=subquestion)\n",
    "            answer = LLM.invoke(rag_prompt)\n",
    "            qa_pairs.append((subquestion, answer))\n",
    "        print(\"Generating QA Pairs: Done!\")\n",
    "\n",
    "        return qa_pairs\n",
    "    \n",
    "    def build_final_answer(question, context, chat_history, qa_pairs, text_date, text_title):\n",
    "        \"\"\"Builds a final answer by integrating the context and QA pairs.\"\"\"\n",
    "        qa_pairs_str = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in qa_pairs])\n",
    "        # final_prompt = prompt_templates.final_rag_template().format(context=context, qa_pairs=qa_pairs_str, question=question)\n",
    "        final_prompt = prompt_templates.final_rag_template_with_memory().format(context=context, qa_pairs=qa_pairs_str, question=question, chat_history=chat_history, text_date=text_date, text_title=text_title)\n",
    "        final_response = LLM.invoke(final_prompt)\n",
    "        print(\"Building Final Answer: Done!\")\n",
    "\n",
    "        return final_response\n",
    "    \n",
    "    subquestions = decompose_question(question)\n",
    "    qa_pairs = generate_qa_pairs(subquestions, text_answers)\n",
    "    print(qa_pairs)\n",
    "    final_answer = build_final_answer(question, text_answers, chat_history, qa_pairs, text_date, text_title)\n",
    "\n",
    "    return output_parser(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938a0aa",
   "metadata": {},
   "source": [
    "# Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3208d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chat_history(user_id, session_id):\n",
    "    \"\"\"\n",
    "    Initializes a chat history object.\n",
    "    \"\"\"\n",
    "    chat_history = []\n",
    "    doc_ref = db.collection(\"chatHistory\").document(user_id).collection(\"session\").document(session_id)\n",
    "    doc_snapshot = doc_ref.get()\n",
    "    try:\n",
    "        if doc_snapshot.exists:\n",
    "            messages = doc_snapshot.get('messages')\n",
    "            if messages is None:\n",
    "                print(f\"No 'messages' field found in document for user_id={user_id}, session_id={session_id}\")\n",
    "                return chat_history\n",
    "            messages = doc_snapshot.get('messages')\n",
    "\n",
    "            for message in messages:\n",
    "                chat_history.append(message)\n",
    "            print(f\"Chat History Initialized: {chat_history}\")\n",
    "        else:\n",
    "            print(f\"No document found for user_id={user_id}, session_id={session_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing chat history: {str(e)}\")\n",
    "    \n",
    "    return chat_history\n",
    "\n",
    "def update_chat_history(user_id, session_id, chat_history):\n",
    "    \"\"\"\n",
    "    Updates the chat history object.\n",
    "    \"\"\"\n",
    "    doc_ref = db.collection(\"chatHistory\").document(user_id).collection(\"session\").document(session_id)\n",
    "    try:\n",
    "        doc_ref.update({\n",
    "            'messages': chat_history\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating chat history: {str(e)}\")\n",
    "\n",
    "def process_chat_history(chat_history):\n",
    "    \"\"\"\n",
    "    Changes the chat history list into a HumanMessages and AIMessages Schema\n",
    "    \"\"\"\n",
    "    process_chat_history = []\n",
    "    for idx, message in enumerate(chat_history):\n",
    "        if idx % 2 == 0:\n",
    "            process_chat_history.append(HumanMessage(message))\n",
    "        else:\n",
    "            process_chat_history.append(AIMessage(message))\n",
    "\n",
    "        \n",
    "    return process_chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaa1da0",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddec182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CHATBOT(query, user_id, session_id, organization):\n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"Current User ID: {user_id}\")\n",
    "    print(f\"Current Session ID: {session_id}\")\n",
    "    print(f\"Organization: {organization}\")\n",
    "    index = pc.Index(organization.lower())\n",
    "\n",
    "    chat_history = initialize_chat_history(user_id=user_id, session_id=session_id)\n",
    "    summaries = fetch_summaries_by_organization(organization=organization)\n",
    "\n",
    "    query_embeddings = get_embeddings(text=query)\n",
    "    meeting_title = resolve_namespace(query=query, query_embeddings=query_embeddings, summaries=summaries)\n",
    "\n",
    "    if meeting_title == \"\":\n",
    "        print(\"AMBIGUOUS MATCH\")\n",
    "        response = generate_followup_question(query, summaries)\n",
    "        return response\n",
    "\n",
    "    text_answers, text_date, text_title = query_pinecone_index(query_embeddings=query_embeddings, meeting_title=meeting_title, index=index)\n",
    "    print(f\"Retrieved context: {text_answers}\\nDate context: {text_date[0]}\\nTitle Context: {text_title[0]}\")\n",
    "\n",
    "    # chat_history \n",
    "\n",
    "    response = decomposition_query_process(question=query, text_answers=text_answers, chat_history=process_chat_history(chat_history), text_date=text_date[0], text_title=text_title[0])\n",
    "\n",
    "    chat_history.append(query)\n",
    "    chat_history.append(response)\n",
    "    update_chat_history(user_id, session_id, chat_history)\n",
    "\n",
    "    print(\"User Query:\", query)\n",
    "    print(\"Chatbot Response:\", response)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daee407",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Did they decide to use the Switchboard model as the starting point for their speaker adaptation?\"\n",
    "user_id = \"WuhmTzwTwmerjkSSK4XT8FyJS263\"\n",
    "session_id = \"session1\"\n",
    "organization = \"SCS\"\n",
    "\n",
    "answer = CHATBOT(query, user_id, session_id, organization)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".backend_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
